# üó∫Ô∏è Google Maps Scraper - Gu√≠a Completa

Script profesional de web scraping para Google Maps que replica la estrategia utilizada por Apify.

## üé¨ Demo

![Demo del Scraper en acci√≥n](demo_scraper.gif)

---

## üìë Tabla de Contenidos

1. [Caracter√≠sticas](#-caracter√≠sticas)
2. [Instalaci√≥n](#-instalaci√≥n)
3. [Composici√≥n del Programa](#-composici√≥n-del-programa)
4. [C√≥mo Usar el Programa](#-c√≥mo-usar-el-programa)
5. [Configuraci√≥n](#-configuraci√≥n)
6. [Datos Extra√≠dos](#-datos-extra√≠dos)
7. [Pausa y Reanudaci√≥n](#-pausa-y-reanudaci√≥n)
8. [Soluci√≥n de Problemas](#-soluci√≥n-de-problemas)
9. [Archivos Generados](#-archivos-generados)
10. [Consideraciones Importantes](#-consideraciones-importantes)

---

## üéØ Caracter√≠sticas

- **Geolocalizaci√≥n inteligente**: Convierte ubicaciones textuales en pol√≠gonos usando Nominatim
- **Divisi√≥n geogr√°fica**: Divide el √°rea en segmentos para cobertura completa
- **B√∫squeda multi-rubro**: Busca m√∫ltiples categor√≠as de negocios
- **Scroll infinito**: Maneja la paginaci√≥n autom√°tica de Google Maps
- **Anti-detecci√≥n**: Usa undetected-chromedriver y comportamiento humano
- **Checkpoints autom√°ticos**: Guarda progreso cada 20 empresas
- **Sistema de Pausa**: Presiona Ctrl+C para pausar de forma segura
- **Campos N/A**: Distingue campos vac√≠os de no buscados
- **Logs detallados**: Sistema de logging estilo Apify
- **Recuperaci√≥n de errores**: Puede retomar desde donde se qued√≥

---

## üõ†Ô∏è Instalaci√≥n

### Requisitos previos

- **Python 3.8+** instalado
- **Google Chrome** instalado
- **Git** (opcional, para clonar el proyecto)

### Paso 1: Crear entorno virtual

```bash
# Navegar al directorio del proyecto
cd Scraper_Maps

# Crear entorno virtual
python3 -m venv scraper

# Activar entorno virtual
# En macOS/Linux:
source scraper/bin/activate

# En Windows:
scraper\Scripts\activate
```

**‚úÖ Ver√°s `(scraper)` al inicio de tu l√≠nea de comandos cuando est√© activado.**

### Paso 2: Instalar dependencias

```bash
pip install -r requirements.txt
```

### Paso 3: Verificar instalaci√≥n

```bash
python -c "import selenium; import undetected_chromedriver; print('‚úÖ Todo OK')"
```

Si ves `‚úÖ Todo OK`, est√°s listo para continuar.

---

## üèóÔ∏è Composici√≥n del Programa

### Archivos Python Principales

El programa est√° compuesto por **7 archivos cr√≠ticos** que trabajan juntos:

#### 1. **`main.py`** - Orquestador Principal
- **Funci√≥n**: Punto de entrada del programa
- **Importa**: config, utils, geolocator, segment_searcher, data_manager
- **Uso**: `python main.py`

#### 2. **`config.py`** - Configuraci√≥n Central
- **Funci√≥n**: Define ubicaciones, rubros, selectores CSS, delays
- **Es usado por**: main.py, segment_searcher.py, detail_extractor.py, completar_telefonos.py
- **Personalizable**: Modifica este archivo para cambiar configuraci√≥n por defecto

#### 3. **`geolocator.py`** - Geolocalizaci√≥n
- **Funci√≥n**: Convierte ubicaciones (ej: "C√≥rdoba, Argentina") en coordenadas y pol√≠gonos
- **API**: Usa Nominatim de OpenStreetMap
- **Divide**: El √°rea en segmentos para cobertura completa

#### 4. **`segment_searcher.py`** - Buscador de Segmentos
- **Funci√≥n**: Realiza b√∫squedas en Google Maps por segmento
- **Maneja**: Scroll infinito, paginaci√≥n, extracci√≥n de resultados
- **Usa**: detail_extractor para obtener datos de cada negocio

#### 5. **`detail_extractor.py`** - Extractor de Detalles
- **Funci√≥n**: Extrae datos de cada negocio (tel√©fono, web, email, etc.)
- **Sistema N/A**: Marca como "N/A" cuando no encuentra un dato
- **Limpieza**: Normaliza y limpia los datos extra√≠dos

#### 6. **`data_manager.py`** - Gestor de Datos
- **Funci√≥n**: Maneja almacenamiento, checkpoints y exportaci√≥n
- **Evita duplicados**: Usa hash √∫nico por negocio
- **Exporta**: A Excel (.xlsx) y CSV

#### 7. **`utils.py`** - Utilidades Comunes
- **Funci√≥n**: Funciones auxiliares (logging, delays, guardado de estado)
- **Es usado por**: Todos los m√≥dulos anteriores

### Archivos Python Auxiliares (Independientes)

#### 8. **`completar_telefonos.py`** - Completar Datos Faltantes
```bash
python completar_telefonos.py
```
Script independiente para volver a buscar tel√©fonos en registros que tengan "N/A".

#### 9. **`analizar_resultados.py`** - An√°lisis de Resultados
```bash
python analizar_resultados.py
```
Genera reportes y estad√≠sticas de los datos extra√≠dos.

#### 10. **`utils_cli.py`** - Herramientas CLI
Utilidades de l√≠nea de comandos para gesti√≥n del proyecto.

#### 11. **`config_example.py`** - Ejemplos de Configuraci√≥n
Archivo de referencia con ejemplos de configuraci√≥n.

### Diagrama de Dependencias

```
main.py (‚òÖ PRINCIPAL)
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ geolocator.py
‚îú‚îÄ‚îÄ segment_searcher.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ utils.py
‚îÇ   ‚îî‚îÄ‚îÄ detail_extractor.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py
‚îÇ       ‚îî‚îÄ‚îÄ utils.py
‚îî‚îÄ‚îÄ data_manager.py
    ‚îî‚îÄ‚îÄ utils.py
```

### Estructura de Directorios

```
Scraper_Maps/
‚îú‚îÄ‚îÄ main.py                      # ‚òÖ Script principal
‚îú‚îÄ‚îÄ config.py                    # ‚òÖ Configuraci√≥n
‚îú‚îÄ‚îÄ geolocator.py               # ‚òÖ Geolocalizaci√≥n
‚îú‚îÄ‚îÄ segment_searcher.py         # ‚òÖ B√∫squeda
‚îú‚îÄ‚îÄ detail_extractor.py         # ‚òÖ Extracci√≥n
‚îú‚îÄ‚îÄ data_manager.py             # ‚òÖ Gesti√≥n de datos
‚îú‚îÄ‚îÄ utils.py                    # ‚òÖ Utilidades
‚îú‚îÄ‚îÄ completar_telefonos.py      # Script auxiliar
‚îú‚îÄ‚îÄ analizar_resultados.py      # Script auxiliar
‚îú‚îÄ‚îÄ utils_cli.py                # Script auxiliar
‚îú‚îÄ‚îÄ config_example.py           # Referencia
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias
‚îú‚îÄ‚îÄ README.md                   # Este archivo
‚îú‚îÄ‚îÄ resultados/                 # Archivos Excel/CSV generados
‚îÇ   ‚îú‚îÄ‚îÄ google_maps_results.xlsx
‚îÇ   ‚îî‚îÄ‚îÄ google_maps_results.csv
‚îú‚îÄ‚îÄ backups/                    # Backups autom√°ticos
‚îú‚îÄ‚îÄ logs/                       # Logs de ejecuci√≥n
‚îú‚îÄ‚îÄ estado_ejecucion.json      # ‚≠ê Estado para recuperaci√≥n (EN GITHUB)
‚îî‚îÄ‚îÄ cookies.pkl                # Cookies de sesi√≥n
```

---

## üöÄ C√≥mo Usar el Programa

### Paso 1: Activar el Entorno Virtual

**IMPORTANTE**: Siempre que vayas a usar el programa, primero activa el entorno virtual:

```bash
# En macOS/Linux:
source scraper/bin/activate

# En Windows:
scraper\Scripts\activate
```

Ver√°s `(scraper)` al inicio de tu terminal cuando est√© activado.

### Paso 2: Ejecutar el Scraper

#### Opci√≥n A: Uso B√°sico (con config.py)

```bash
python main.py
```

Esto usar√° la configuraci√≥n definida en `config.py`:
- Ubicaci√≥n por defecto
- Rubros por defecto
- Grid size por defecto

#### Opci√≥n B: Personalizar desde L√≠nea de Comandos

```bash
# Cambiar ubicaci√≥n
python main.py --ubicacion "Buenos Aires, Argentina"

# Cambiar rubros
python main.py --rubros "restaurante" "hotel" "gimnasio"

# Cambiar tama√±o de grid (1=1x1, 2=2x2, 3=3x3, etc.)
python main.py --grid-size 2

# Modo headless (sin ventana visible)
python main.py --headless

# Combinaci√≥n completa
python main.py --ubicacion "Rosario, Argentina" --rubros "fabrica" "logistica" --grid-size 2
```

### Ejemplos Pr√°cticos

#### Ejemplo 1: Buscar restaurantes en Buenos Aires
```bash
python main.py --ubicacion "Buenos Aires, Argentina" --rubros "restaurante" "bar" "cafeter√≠a"
```

#### Ejemplo 2: Buscar f√°bricas en C√≥rdoba (cobertura 2x2)
```bash
python main.py --ubicacion "C√≥rdoba, Argentina" --rubros "fabrica" "industria" --grid-size 2
```

#### Ejemplo 3: B√∫squeda exhaustiva (cobertura 3x3)
```bash
python main.py --ubicacion "Mendoza, Argentina" --rubros "hotel" "restaurante" --grid-size 3
```

### Paso 3: Monitorear el Progreso

El programa muestra logs en tiempo real:

```
2024-01-24T10:30:00 INFO  üì° Geolocalizando: C√≥rdoba, Argentina
2024-01-24T10:30:02 INFO  ‚úÖ Ubicaci√≥n encontrada
2024-01-24T10:30:02 INFO  üìê Dividiendo √°rea en cuadr√≠cula de 2x2
2024-01-24T10:30:02 INFO  ‚úÖ Creados 4 segmentos
2024-01-24T10:30:05 INFO  üîç Buscando 'fabrica' en segmento 0
2024-01-24T10:30:45 INFO  üîç [fabrica][-31.4201|-64.1888][SCROLL: 8]: Search page scraped: 42 unique, 5 duplicate
2024-01-24T10:30:45 INFO  üìä 42 lugares agregados (Total: 42)
```

**Para ver logs en tiempo real en otra terminal:**
```bash
tail -f logs/scraper_*.log
```

### Paso 4: Ver Resultados

Los resultados se guardan autom√°ticamente en:
- **Excel principal**: `resultados/google_maps_results.xlsx`
- **CSV**: `resultados/google_maps_results.csv`
- **Backups**: `backups/backup_TIMESTAMP.xlsx`

Puedes abrir estos archivos mientras el programa est√° ejecutando.

### Paso 5: Desactivar el Entorno Virtual (cuando termines)

```bash
deactivate
```

---

## ‚öôÔ∏è Configuraci√≥n

### M√©todo 1: Editar `config.py`

Abre `config.py` con tu editor favorito y modifica:

```python
CONFIG = {
    # Ubicaci√≥n a scrapear
    'ubicacion': "C√≥rdoba, Argentina",
    
    # Rubros/categor√≠as a buscar
    'rubros': ["fabrica", "logistica", "transportes"],
    
    # Tama√±o de la cuadr√≠cula (2 = 2x2 = 4 segmentos)
    'grid_size': 2,
    
    # Nivel de zoom en Google Maps
    'zoom_level': 13,
    
    # Guardar checkpoint cada N empresas
    'checkpoint_cada': 20,
    
    # Delays entre acciones (segundos)
    'delays': {
        'entre_segmentos': (8, 15),
        'entre_rubros': (4, 8),
        'despues_scroll': (2, 4),
        'despues_click': (1, 2)
    },
    
    # L√≠mites de seguridad
    'max_scrolls': 20,
    'max_empresas_por_dia': 2000,
    
    # Configuraci√≥n del navegador
    'headless': False,  # True = sin ventana visible
    'user_data_dir': None
}
```

### M√©todo 2: Argumentos de L√≠nea de Comandos

```bash
python main.py --help
```

Muestra todas las opciones disponibles:
- `--ubicacion`: Ubicaci√≥n a scrapear
- `--rubros`: Lista de rubros
- `--grid-size`: Tama√±o de cuadr√≠cula (1, 2, 3, 4...)
- `--headless`: Modo sin ventana visible

---

## üìä Datos Extra√≠dos

Para cada negocio/lugar extrae:

| Campo | Descripci√≥n | Ejemplo |
|-------|-------------|---------|
| `nombre` | Nombre del lugar | "Restaurante El Buen Sabor" |
| `direccion` | Direcci√≥n completa | "Av. Col√≥n 123, C√≥rdoba" |
| `categoria` | Categor√≠a del negocio | "Restaurante" |
| `rating` | Puntuaci√≥n | "4.5" |
| `num_resenas` | N√∫mero de rese√±as | "127" |
| `telefono` | Tel√©fono | "+54 351 123-4567" o "N/A" |
| `sitio_web` | Sitio web | "www.ejemplo.com" o "N/A" |
| `email` | Email | "info@ejemplo.com" o "N/A" |
| `url_google_maps` | URL de Google Maps | "https://maps.google.com/..." |
| `latitud` | Coordenada latitud | "-31.4201" |
| `longitud` | Coordenada longitud | "-64.1888" |
| `rubro_buscado` | Rubro que se busc√≥ | "fabrica" |
| `segmento_id` | ID del segmento | "0" |
| `fecha_extraccion` | Fecha y hora | "2024-01-24 10:30:00" |

### Sistema N/A

El programa distingue entre:
- **"N/A"**: El dato fue buscado pero NO se encontr√≥
- **Vac√≠o**: El dato no fue buscado todav√≠a

Esto es √∫til para saber qu√© datos realmente no existen vs. cu√°les faltan procesar.

---

## ‚è∏Ô∏è Pausa y Reanudaci√≥n

### Pausar el Scraper

**Presiona `Ctrl+C` UNA SOLA VEZ** en la terminal donde est√° ejecutando el programa.

```
‚è∏Ô∏è  PAUSA SOLICITADA - Guardando estado...
El scraper se detendr√° despu√©s de completar el elemento actual.
NO presiones Ctrl+C nuevamente, espera a que termine de guardar.
```

El programa:
1. Termina de procesar el elemento actual
2. Guarda todos los datos
3. Guarda el estado en `estado_ejecucion.json`
4. Se cierra limpiamente

### Reanudar el Scraper

Simplemente ejecuta de nuevo:

```bash
python main.py
```

El programa:
1. Detecta el archivo `estado_ejecucion.json`
2. Carga el progreso anterior
3. **Contin√∫a EXACTAMENTE donde se qued√≥**
4. **NO repite rubros ya procesados**

### üñ•Ô∏è Continuar desde Otra Computadora

**IMPORTANTE**: El archivo `estado_ejecucion.json` est√° en GitHub para permitir portabilidad.

Para continuar el scraping desde otra computadora:

1. **En la computadora original** (donde pausaste):
   ```bash
   # Aseg√∫rate de subir el estado actualizado
   git add estado_ejecucion.json resultados/
   git commit -m "Actualizar estado de ejecuci√≥n"
   git push origin main
   ```

2. **En la computadora nueva**:
   ```bash
   # Clonar el repositorio
   git clone https://github.com/panasabena/Google_Maps_Scraper.git
   cd Google_Maps_Scraper
   
   # Activar entorno virtual e instalar dependencias
   python3 -m venv scraper
   source scraper/bin/activate
   pip install -r requirements.txt
   
   # Ejecutar - continuar√° autom√°ticamente desde donde se qued√≥
   python main.py
   ```

3. **El programa autom√°ticamente**:
   - Lee `estado_ejecucion.json` del repositorio
   - Carga los 25,000+ registros ya extra√≠dos
   - **NO repite ubicaciones/rubros completados**
   - Contin√∫a con los rubros/segmentos pendientes

### Empezar desde Cero

Si quieres comenzar una nueva extracci√≥n desde el principio:

```bash
rm estado_ejecucion.json
rm resultados/*.xlsx
rm resultados/*.csv
python main.py
```

---

## üêõ Soluci√≥n de Problemas

### Error: "ModuleNotFoundError: No module named 'X'"

**Causa**: El entorno virtual no est√° activado o las dependencias no est√°n instaladas.

**Soluci√≥n**:
```bash
source scraper/bin/activate  # Activar entorno
pip install -r requirements.txt  # Instalar dependencias
```

### Error: "ChromeDriver not found" o "Chrome not found"

**Causa**: Google Chrome no est√° instalado.

**Soluci√≥n**:
- **macOS**: Descarga desde https://www.google.com/chrome/
- **Linux**: `sudo apt install google-chrome-stable`
- **Windows**: Descarga desde https://www.google.com/chrome/

### Error: "Shapely no funciona"

**Causa**: Falta librer√≠a GEOS del sistema.

**Soluci√≥n**:
```bash
# En macOS
brew install geos

# En Ubuntu/Debian
sudo apt-get install libgeos-dev

# Reinstalar Shapely
pip uninstall shapely
pip install shapely --no-binary shapely
```

### El navegador se cierra inmediatamente

**Posibles causas**:
1. Chrome no est√° instalado correctamente
2. Problema con undetected-chromedriver
3. Error en la configuraci√≥n

**Soluci√≥n**:
```bash
# Ejecutar sin headless para ver qu√© pasa
python main.py  # Sin --headless

# Verificar logs
cat logs/scraper_*.log
```

### No se encuentran resultados (0 lugares extra√≠dos)

**Posibles causas**:
1. Ubicaci√≥n inv√°lida o mal escrita
2. Selectores CSS desactualizados (Google cambi√≥ su HTML)
3. Google detect√≥ el scraper
4. Sin conexi√≥n a internet

**Soluci√≥n**:
```bash
# 1. Verificar la ubicaci√≥n
python main.py --ubicacion "Buenos Aires, Argentina"  # Usar nombre completo

# 2. Verificar conexi√≥n
ping google.com

# 3. Aumentar delays en config.py
'delays': {
    'entre_segmentos': (15, 25),  # Aumentar
    'entre_rubros': (8, 15),       # Aumentar
    'despues_scroll': (3, 6)       # Aumentar
}

# 4. Revisar logs
cat logs/scraper_*.log
```

### Error: "selenium.common.exceptions.TimeoutException"

**Causa**: El elemento no se encontr√≥ en el tiempo esperado.

**Soluci√≥n**:
1. Aumentar delays en `config.py`
2. Verificar conexi√≥n a internet
3. Ejecutar sin headless para ver qu√© pasa
4. Los selectores pueden estar desactualizados

### El programa se queda "colgado" en un punto

**Posibles causas**:
1. Scroll infinito sin fin
2. P√°gina cargando muy lento
3. Popup o modal bloqueando

**Soluci√≥n**:
```bash
# Presiona Ctrl+C para pausar
Ctrl+C

# Revisa los logs
cat logs/scraper_*.log

# Reduce max_scrolls en config.py
'max_scrolls': 10  # En lugar de 20
```

---

## üìÅ Archivos Generados

### Archivos de Resultados

#### `resultados/google_maps_results.xlsx`
- Archivo Excel principal con todos los datos
- Se actualiza cada 20 empresas (checkpoint)
- Contiene todas las columnas de datos

#### `resultados/google_maps_results.csv`
- Versi√≥n CSV de los resultados
- Mismo contenido que el Excel
- √ötil para importar a otras herramientas

#### `backups/backup_YYYYMMDD_HHMMSS.xlsx`
- Backups autom√°ticos con timestamp
- Se crean en cada checkpoint
- Formato: `backup_20240124_103000.xlsx`

### Archivos de Estado

#### `estado_ejecucion.json` ‚≠ê IMPORTANTE

- **Guarda el progreso completo del scraper**
- **Permite reanudar desde donde se qued√≥**
- **Esencial para continuar desde otra computadora**
- **Evita repetir 25,000+ registros ya extra√≠dos**
- **Est√° en GitHub para portabilidad**

**Contenido:**
- Ubicaciones completadas
- Rubros procesados por ubicaci√≥n
- Segmentos finalizados
- Total de empresas extra√≠das
- Timestamp del √∫ltimo checkpoint
- Fecha de inicio

**Ejemplo:**
```json
{
  "ubicaciones_completadas": {
    "buenos_aires_argentina": {
      "nombre": "Buenos Aires, Argentina",
      "rubros_completados": [
        "fabrica",
        "logistica",
        "transportes"
      ],
      "segmentos_completados": {
        "seg_0": {
          "rubros": ["fabrica", "logistica"],
          "completado": true
        }
      }
    }
  },
  "empresas_extraidas": 25507,
  "ultimo_checkpoint": "2024-01-26T00:23:00",
  "fecha_inicio": "2024-01-24T09:00:00"
}
```

**üí° Uso entre computadoras:**
1. Commit y push del estado: `git add estado_ejecucion.json && git commit -m "Update" && git push`
2. En otra PC: `git pull` y ejecuta `python main.py`
3. El programa contin√∫a autom√°ticamente sin repetir datos

#### `cookies.pkl`
- Cookies de sesi√≥n de Google
- Evita re-autenticaci√≥n
- Formato pickle (binario)

### Archivos de Logs

#### `logs/scraper_YYYYMMDD_HHMMSS.log`
- Logs detallados de ejecuci√≥n
- Un archivo por ejecuci√≥n
- Formato: `scraper_20240124_103000.log`
- Codificaci√≥n: UTF-8

Ejemplo de logs:
```
2024-01-24T10:30:00 INFO  üì° Iniciando scraper...
2024-01-24T10:30:00 INFO  üìç Ubicaci√≥n: C√≥rdoba, Argentina
2024-01-24T10:30:00 INFO  üè∑Ô∏è  Rubros: fabrica, logistica
2024-01-24T10:30:02 INFO  ‚úÖ Geolocalizaci√≥n exitosa
2024-01-24T10:30:02 INFO  üìê Creados 4 segmentos
2024-01-24T10:30:05 INFO  üîç Buscando 'fabrica' en segmento 0
```

---

## ‚ö†Ô∏è Consideraciones Importantes

### L√≠mites de Google Maps

- Google puede detectar y bloquear scraping excesivo
- Usa delays apropiados entre solicitudes
- No ejecutes el script 24/7
- Considera usar la API oficial de Google Places para uso comercial

### Uso Responsable

- Este script es para **fines educativos**
- Respeta los t√©rminos de servicio de Google
- No sobrecargues los servidores de Google
- Usa los datos de manera √©tica

### Anti-Detecci√≥n

El script incluye:
- ‚úÖ User-Agent rotation
- ‚úÖ Delays aleatorios
- ‚úÖ undetected-chromedriver
- ‚úÖ Comportamiento similar al humano

A√∫n as√≠, Google puede detectarlo. Para uso en producci√≥n considera:
- Proxies rotativos
- Distribuci√≥n de IPs
- API oficial de Google Places

### Recomendaciones de Uso

#### Por Tama√±o de B√∫squeda

| Caso de Uso | Grid Size | Rubros | Tiempo Estimado |
|-------------|-----------|--------|-----------------|
| Prueba r√°pida | 1 (1x1) | 1-2 | 10-20 min |
| B√∫squeda normal | 2 (2x2) | 2-3 | 30-60 min |
| B√∫squeda exhaustiva | 3 (3x3) | 3-5 | 2-4 horas |
| Muy exhaustiva | 4 (4x4) | 5+ | 4-8 horas |

#### Tips Importantes

1. **Empieza peque√±o**: Prueba con 1-2 rubros y grid-size 1 primero
2. **Monitorea los logs**: Te dir√°n exactamente qu√© est√° pasando
3. **S√© paciente**: Google Maps puede ser lento
4. **Usa delays apropiados**: No hagas scraping agresivo
5. **Backups autom√°ticos**: No los borres, son tu seguro
6. **Horarios**: Ejecuta preferiblemente en horarios de bajo tr√°fico

### Legalidad y √âtica

‚ö†Ô∏è **IMPORTANTE**: 

- Este software es para **fines educativos y de investigaci√≥n**
- El web scraping puede violar los T√©rminos de Servicio de Google
- Google puede bloquear tu IP si detecta actividad de scraping
- Para uso comercial, **usa la API oficial de Google Places**
- Los datos extra√≠dos son de dominio p√∫blico pero tienen derechos
- Usa bajo tu propia responsabilidad

---

## üìà Proceso de Ejecuci√≥n

### Flujo del Programa

```
1. INICIO
   ‚Üì
2. Cargar configuraci√≥n (config.py o argumentos)
   ‚Üì
3. Inicializar navegador Chrome (undetected-chromedriver)
   ‚Üì
4. GEOLOCALIZACI√ìN
   - Consultar Nominatim API
   - Obtener pol√≠gono de la ubicaci√≥n
   - Calcular bounding box
   ‚Üì
5. SEGMENTACI√ìN
   - Dividir √°rea en cuadr√≠cula (grid_size x grid_size)
   - Calcular centro de cada segmento
   ‚Üì
6. B√öSQUEDA (por cada segmento)
   - Para cada rubro:
     a. Construir URL de b√∫squeda
     b. Navegar a Google Maps
     c. Manejar pantallas de consentimiento
     d. SCROLL INFINITO
        - Hacer scroll hacia abajo
        - Esperar carga de elementos
        - Extraer datos de cada lugar
        - Filtrar duplicados
        - Repetir hasta fin de resultados
     e. Guardar datos
   - Checkpoint cada 20 empresas
   ‚Üì
7. EXPORTACI√ìN FINAL
   - Crear DataFrame con todos los datos
   - Exportar a Excel (.xlsx)
   - Exportar a CSV (.csv)
   - Crear backup final
   ‚Üì
8. FIN
   - Cerrar navegador
   - Mostrar estad√≠sticas
```

### Tiempo Estimado

```
Por lugar:              ~1-2 segundos
Por p√°gina (20 lugares): ~40-80 segundos
Por rubro (100 lugares): ~5-10 minutos
Por segmento (3 rubros): ~15-30 minutos
Total (4 seg, 3 rubros): ~1-2 horas
```

---

## üìû Soporte y Ayuda

### Revisar Logs

Los logs contienen informaci√≥n detallada de lo que est√° pasando:

```bash
# Ver √∫ltimo log
cat logs/scraper_*.log | tail -100

# Ver logs en tiempo real
tail -f logs/scraper_*.log

# Buscar errores
grep ERROR logs/scraper_*.log
```

### Verificar Estado

```bash
# Ver archivo de estado
cat estado_ejecucion.json

# Ver resultados parciales
ls -lh resultados/
```

### Limpiar Proyecto

Para empezar desde cero y limpiar todo:

```bash
# Eliminar estado
rm estado_ejecucion.json

# Eliminar resultados
rm -rf resultados/*.xlsx resultados/*.csv

# Eliminar backups
rm -rf backups/*.xlsx

# Eliminar logs antiguos
rm -rf logs/*.log

# Eliminar cookies
rm cookies.pkl
```

---

## ü§ù Mejoras Futuras

√Åreas de mejora identificadas:

- [ ] Extracci√≥n desde `APP_INITIALIZATION_STATE` (m√°s r√°pido)
- [ ] Paralelizaci√≥n por segmentos
- [ ] Sistema de proxies rotativos
- [ ] Base de datos (PostgreSQL) en lugar de Excel
- [ ] API REST para control remoto
- [ ] Dashboard web en tiempo real
- [ ] Soporte multi-idioma
- [ ] Detecci√≥n autom√°tica de bloqueos
- [ ] Rate limiting adaptativo
- [ ] Exportaci√≥n a otros formatos (JSON, SQL, etc.)

---

## üìù Licencia

Este proyecto es para fines educativos. Usa bajo tu propia responsabilidad.

**NO se ofrece garant√≠a de ning√∫n tipo.**

---

## ‚úÖ Checklist de Uso R√°pido

- [ ] Python 3.8+ instalado
- [ ] Google Chrome instalado
- [ ] Entorno virtual creado: `python3 -m venv scraper`
- [ ] Entorno activado: `source scraper/bin/activate`
- [ ] Dependencias instaladas: `pip install -r requirements.txt`
- [ ] Configuraci√≥n revisada en `config.py`
- [ ] Ejecutar: `python main.py`
- [ ] Monitorear logs: `tail -f logs/scraper_*.log`
- [ ] Revisar resultados en `resultados/google_maps_results.xlsx`

---

**‚ö° Happy Scraping!**

---

*√öltima actualizaci√≥n: Enero 2024*
